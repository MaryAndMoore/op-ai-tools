# Project Overview - First Milestone

This project aims to create a complete AI solution, empowering individual users to access reliable information about Optimism. In this document, we present the progress made in the first milestone.

As the goal of this first part was to conceive a chatbot web application for users to interact by asking questions, our approach was to implement a Retrieval-Augmented Generation (RAG) system. This system combines a retriever architecture with a Large Language Model (LLM) to provide accurate and context-aware answers to user queries.

**First Proof of Concept**

We wanted to test the feasibility of our approach. The first proof of concept was to build a simple system that could retrieve and generate responses to questions about Optimism. The idea was to have a quick implementation that could be iterated upon.

- We have two main sources: the Optimism Governance Documentation and the Optimism Governance Forum. Our initial step was to download and locally store the data.
- We then used `LangChain` to clean and process the data into chunks that could be used by the system.
- Using the `OpenAI API`, we generated embeddings for the processed data and stored them locally using the `FAISS` (Facebook AI Similarity Search) for efficient retrieval.
- The `OpenAI API` was also used to access the LLM for generating responses to user queries.

The choice of these tools was based on their compatibility, performance, and ease of use. As we were aiming for a quick implementation, we prioritized straightforward integration and minimal setup. There are some [options](https://openai.com/api/pricing/) regarding the `OpenAI API` language and embedding models. In the first POC, we used the smallest ones to reduce costs. The idea was to have a baseline to compare with more complex models in the future.

**Testing Data**

Once the POC was functional, it was important to evaluate its performance. Since we didn't have a ready-made test dataset, we had to create one.

Using the same tools in a sort of reverse process, we iterated over the data chunks to generate questions and answers. We employed similarity techniques in the embedding space to filter out redundant or irrelevant questions. Finally, our team manually reviewed these to classify the relevance of the generated questions. The reference answers were generated by GPT-4o and submitted to the Optimism team for review.

**Metrics**

In the test dataset, let $q$ be a question and $e$ the expected answer (sometimes referred to as ground truth). We have a retriever $ret$ and a language model $llm$ such that the context is given by $c = ret(q)$ and the answer is given by $a = llm(c, q)$. The complete system is defined as $rag(q) = llm(ret(q), q) = a$. We want metrics that allow the evaluation of the retriever, the language model, and the complete RAG system, end-to-end. Here are the metrics we defined:

- **Evaluating the Retriever**
  - **Context Recall:** We want to ensure that the retriever finds the correct information, meaning that the ground truth's meaning is contained within the context's meaning. Given a meaning operator $M$, we want $M(e) \subset M(c)$. We assume that meaning is relatively well-represented by the projection of strings in the embedding space. However, there is no trivial way to represent set operations. The solution, taken from the [RAGAS paper](https://aclanthology.org/2024.eacl-demo.16.pdf), is to define a prompt (using an LLM) $S$ that extracts atomic sentences contained in the expected answer and to check (also using an LLM) if these atomic sentences are semantically contained in the context. Formally, ask the chat for $S = \{s_1, …, s_n\}$ such that $M(e) \approx M(S)$ and return the proportion of $s_i \in S$ where $M(s_i) \in M(c)$.
  - **Context Conciseness:** Ideally, we want a context that does not return too much unnecessary information, for performance reasons. We define context conciseness as the proportion $\frac{numTokens(e)}{numTokens(c)}$ (the higher the conciseness, the better).

- **Evaluating the LLM**
  - **Answer Relevance:** To measure how much the answer seems to be addressing the question. Also taken from the RAGAS paper. We use another language model that, based on the answer $a$, gives us the “question that seems to be being answered” $\tilde{q}(a)$ and checks the semantic similarity (simply the cosine similarity in the embedding space). That is, we check if $M(\tilde{q}(a)) \approx M(q)$.
  - **Faithfulness:** We want to ensure that the LLM only shows information present in the context. That is, the meaning of the answer should be contained within the context's meaning. Given a meaning operator $M$, we want $M(a) \subset M(c)$. We do this similarly to context recall.

- **Evaluating the RAG End-to-End**
  - **Answer Semantic Similarity:** The same as used to create the test dataset. Cosine similarity in the embedding space to ensure $M(a) \approx M(e)$.
  - **Answer Conciseness:** Same idea as for the context. We do not want unnecessarily long answers. We want, while maintaining semantic requirements, to maximize the proportion $\frac{numTokens(e)}{numTokens(a)}$.

When comparing the systems in the following, we try to encompass all these metrics.

**Experimentation**

Establishing the metrics, we could experiment with some different configurations to refine our model. An overview of these variations is:

- We tested the different OpenAI embedding models. The best performance was achieved with the `ada v2`.
- Regarding the LLM, we tested the available OpenAI's GPT and Anthropic's Claude. The latter showed better results.
- We varied the architecture of the system. Some of the configurations tested were:
    - Multi-Retriever: Instead of using a single retriever that combines the two sources, we used two retrievers, one for each source. The idea was to test if the LLM could benefit from assuring to always have both information from the forum and the documentation. The results were similar to the single retriever system.
    - Contextual Compression: As using the LLM has some limitations regarding the amount of text it can process in a single call without losses, we tested a strategy of retrieving more information than the LLM could handle and compressing it into a single piece of text. The results were mixed, as for more specific questions, the compression could lose relevant information; for more general questions, the compression helped the LLM to focus on the most relevant information. Some more tests are needed to refine this strategy.
    - Query Expander: The system faced a recurrent problem of misunderstanding "too general" questions. For example, when asked "What is Optimism?", the retriever would return forum entries of the word optimism being used in a different context of positive thinking, compromising the LLM response. We tested adding another layer, before the retriever, that would expand the query with additional context (in this case, it could be related questions such as "how Optimism works?", "how Optimism is related to Ethereum?"...). The results were promising, allowing the system to handle these cases better.

The implementation we made available for testing relies on the best configurations we found, with the `ada v2` embedding model, Anthropic's Claude LLM, and the Query Expander layer. Further work may focus on refining the explored architectures, testing new ones, as well as improving prompt engineering and data preprocessing.
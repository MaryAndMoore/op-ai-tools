{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for exploring variations on LLM, embeddings, RAG arch and prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBs that are going to be retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs = [\n",
    "    \"full_docs\",\n",
    "    \"fragments_docs\",\n",
    "    \"posts_forum\",\n",
    "    \"threads_forum\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options of embedding, chat and vectorscore to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to test the open ai api for embeddings\n",
    "embedding_models = [\"text-embedding-3-small\", \"text-embedding-3-large\", \"text-embedding-ada-002\"]\n",
    "\n",
    "# we are going to test the open ai api for chat\n",
    "chat_models = [\"gpt-3.5-turbo-0125\", \"gpt-4o\"]\n",
    "\n",
    "# we are going to test the anthropic api for chat\n",
    "chat_models_claude = [\"claude-3-sonnet-20240229\"]\n",
    "\n",
    "# we are going to use faiss\n",
    "vectorstores = ['faiss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, asyncio, time, re\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "import tiktoken # metrics\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from typing import Callable\n",
    "\n",
    "# embedding and chat\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# openai api key\n",
    "openai_api_key = getpass(\"Enter the OpenAI API key: \")\n",
    "\n",
    "# Anthropic chat\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = getpass(\"Enter your Anthropic API key: \")\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# vectorstore\n",
    "if 'faiss' in vectorstores:\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# for tracking\n",
    "import weave\n",
    "from weave import Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"../002_create_test_dataset/questions_test_dataset.csv\"\n",
    "test_dataset = pd.read_csv(test_path)\n",
    "\n",
    "# drop origin\n",
    "#test_dataset = test_dataset.drop(columns=['origin'])\n",
    "\n",
    "# change columns\n",
    "test_dataset = test_dataset.rename(columns={'answer': 'expected'})\n",
    "\n",
    "# sample 50 questions\n",
    "test_dataset = test_dataset.sample(50, random_state=42)\n",
    "\n",
    "# as dict\n",
    "test_dataset = test_dataset.to_dict(orient='records')\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General definitions for accessing data and creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db(dbs, model_embeddings, vectorstore = 'faiss'):\n",
    "    embeddings = OpenAIEmbeddings(model=model_embeddings, openai_api_key=openai_api_key)\n",
    "    if vectorstore == 'faiss':\n",
    "        dbs = [f\"dbs/{name}_db/faiss/{model_embeddings}\" for name in dbs]\n",
    "        dbs = [FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True) for db_path in dbs]\n",
    "        db = dbs[0]\n",
    "        for db_ in dbs[1:]:\n",
    "            db.merge_from(db_)\n",
    "    \n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def build_chat(chat_pars, prompt_template):\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "    llm = ChatOpenAI(**chat_pars, openai_api_key=openai_api_key)\n",
    "    chain = prompt | llm\n",
    "\n",
    "    return chain, llm\n",
    "    \n",
    "@weave.op()\n",
    "def build_retriever(dbs_name, embeddings_name, vectorstore = 'faiss', retriever_pars = {}):\n",
    "    db = load_db(dbs_name, embeddings_name, vectorstore)\n",
    "    if vectorstore == 'faiss':\n",
    "        retriever = db.as_retriever(**retriever_pars)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "class RAGModel(weave.Model):\n",
    "    structure : str = \"simple-rag\" # just a retriever and a llm\n",
    "\n",
    "    dbs_name : list\n",
    "    embeddings_name : str\n",
    "\n",
    "    vectorstore : str\n",
    "    retriever_pars : dict\n",
    "\n",
    "    prompt_template : str\n",
    "    chat_pars : dict[str, str|int]\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str):\n",
    "        retriever = build_retriever(self.dbs_name, self.embeddings_name, self.vectorstore, self.retriever_pars)\n",
    "        chain, llm = build_chat(self.chat_pars, self.prompt_template)\n",
    "\n",
    "        if self.vectorstore == 'faiss':\n",
    "            context = retriever.invoke(question)\n",
    "\n",
    "        response = chain.invoke(\n",
    "            {\n",
    "                \"context\": context,\n",
    "                \"question\": question,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return {\"context\": str(context), \"answer\": response.content}\n",
    "    \n",
    "    def ask(self, question: str):\n",
    "        out = self.predict(question)\n",
    "        return out[\"answer\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference embedding\n",
    "reference_embedding = \"text-embedding-ada-002\"\n",
    "reference_embedding = OpenAIEmbeddings(model=reference_embedding, openai_api_key=openai_api_key)\n",
    "\n",
    "# reference chat\n",
    "reference_chat = \"gpt-4o\"\n",
    "reference_chat = ChatOpenAI(model=reference_chat, openai_api_key=openai_api_key)\n",
    "\n",
    "# reference tokenization\n",
    "reference_tokenization = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE THE RETRIEVER\n",
    "def calc_context_recall(context, question, expected):\n",
    "    # measure if the expected meaning is contained in the context meaning\n",
    "    # info: https://aclanthology.org/2024.eacl-demo.16.pdf\n",
    "\n",
    "    # we try to find a set of fundamental statements that encompass the meaning of the answer\n",
    "    statements = reference_chat.invoke(\n",
    "        f\"Given a question and answer, return the fundamental statements from the answer's meaning. \\n question: {question} \\n answer: {expected}\"\n",
    "    ).content\n",
    "    # we try to find if the statements are supported by the context\n",
    "    veredicts = reference_chat.invoke(\n",
    "        f\"Consider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explanation for each statement before arriving at the verdict (Yes/No). Let the veredict be the final word of each line. \\n\\n <context> \\n {context} <\\\\context> \\n\\n <statements> \\n {statements} \\n <\\\\statements>\"\n",
    "    ).content\n",
    "\n",
    "    # get the last word of each line\n",
    "    veredicts = veredicts.split(\"\\n\")\n",
    "    veredicts = [veredict.split(\" \")[-1].lower() for veredict in veredicts]\n",
    "    n_yes = 0\n",
    "    n_no = 0\n",
    "    for veredict in veredicts:\n",
    "        if \"yes\" in veredict:\n",
    "            n_yes += 1\n",
    "        elif \"no\" in veredict:\n",
    "            n_no += 1\n",
    "        else:\n",
    "            None\n",
    "\n",
    "    try:\n",
    "        recall = n_yes / (n_yes + n_no)\n",
    "    except ZeroDivisionError:\n",
    "        recall = 0\n",
    "\n",
    "    return recall\n",
    "\n",
    "def calc_context_conciseness(context, expected):\n",
    "    # measure the size of the context compared to the expected answer\n",
    "    num_tokens_context = len(reference_tokenization.encode(context))\n",
    "    num_tokens_expected = len(reference_tokenization.encode(expected))\n",
    "\n",
    "    return num_tokens_expected / num_tokens_context\n",
    "\n",
    "# EVALUATE THE LLM\n",
    "def calc_answer_relevance(answer, question):\n",
    "    # measure how much the answer resembles to be answering the question\n",
    "    # info: https://aclanthology.org/2024.eacl-demo.16.pdf\n",
    "    hipot_question = reference_chat.invoke(\n",
    "        f\"Generate a question for the given answer. \\n answer: {answer}\"\n",
    "    ).content\n",
    "    hipot_question_embedding = np.array(reference_embedding.embed_query(hipot_question))\n",
    "    question_embedding = np.array(reference_embedding.embed_query(question))\n",
    "    return np.dot(hipot_question_embedding, question_embedding) / (np.linalg.norm(hipot_question_embedding) * np.linalg.norm(question_embedding))\n",
    "\n",
    "def calc_faithfulness(context, question, answer):\n",
    "    # measure if the answer's meaning is contained in the context meaning\n",
    "    # info: https://aclanthology.org/2024.eacl-demo.16.pdf\n",
    "    return calc_context_recall(context, question, answer)\n",
    "\n",
    "\n",
    "# EVALUATE END-TO-END\n",
    "def calc_answer_semantic_similarity(answer, expected):\n",
    "    # measure similarity between the answer and the expected answer\n",
    "    # info: https://docs.ragas.io/en/latest/concepts/metrics/semantic_similarity.html\n",
    "    answer_embedding = np.array(reference_embedding.embed_query(answer))\n",
    "    expected_embedding = np.array(reference_embedding.embed_query(expected))\n",
    "\n",
    "    return np.dot(answer_embedding, expected_embedding) / (np.linalg.norm(answer_embedding) * np.linalg.norm(expected_embedding))\n",
    "\n",
    "def calc_answer_conciseness(answer, expected):\n",
    "    # measure the size of the answer compared to the expected answer\n",
    "    num_tokens_answer = len(reference_tokenization.encode(answer))\n",
    "    num_tokens_expected = len(reference_tokenization.encode(expected))\n",
    "\n",
    "    return num_tokens_expected / num_tokens_answer\n",
    "\n",
    "# ALL\n",
    "def calc_metrics(question, expected, context, answer):\n",
    "    return {\n",
    "        \"retriever\": {\n",
    "            \"context_recall\": calc_context_recall(context, question, expected),\n",
    "            \"context_conciseness\": calc_context_conciseness(context, expected)\n",
    "        },\n",
    "        \"llm\": {\n",
    "            \"answer_relevance\": calc_answer_relevance(answer, question),\n",
    "            \"faithfulness\": calc_faithfulness(context, question, answer)\n",
    "        },\n",
    "        \"end-to-end\": {\n",
    "            \"answer_semantic_similarity\": calc_answer_semantic_similarity(answer, expected),\n",
    "            \"answer_conciseness\": calc_answer_conciseness(answer, expected)\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def eval_model(question: str, expected: str, model_output: dict) -> dict:\n",
    "    return calc_metrics(question, expected, model_output[\"context\"], model_output[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_evaluation(rag, test_dataset=test_dataset):\n",
    "    evaluation = Evaluation(\n",
    "        dataset=test_dataset, scorers=[eval_model],\n",
    "    )\n",
    "\n",
    "    with weave.attributes({'dbs': rag.dbs_name, 'embeddings': rag.embeddings_name, 'chat_pars': rag.chat_pars, 'prompt_template': rag.prompt_template, 'retriever_pars': rag.retriever_pars, 'vectorstore': rag.vectorstore, \"structure\": rag.structure}):\n",
    "        asyncio.run(evaluation.evaluate(rag.predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.init('op-ai-tools')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_pars = {\n",
    "    \"model\": chat_models[1],\n",
    "    \"temperature\": 0,\n",
    "    #\"max_tokens\": None,\n",
    "    #\"timeout\": None,\n",
    "    \"max_retries\": 2\n",
    "}\n",
    "\n",
    "prompt_template = f\"\"\"Answer politely the question at the end, using only the following context. The user is not necessarily a specialist, so please avoid jargon and explain any technical terms.\n",
    "\n",
    "<context>\n",
    "{{context}} \n",
    "</context>\n",
    "\n",
    "Question: {{question}}\n",
    "\"\"\"\n",
    "\n",
    "rag = RAGModel(\n",
    "    dbs_name = dbs[1:3],\n",
    "    embeddings_name = embedding_models[2],\n",
    "    chat_pars=chat_pars,\n",
    "    prompt_template = prompt_template,\n",
    "    retriever_pars = {\n",
    "        \"search_kwargs\" : {'k': 6}\n",
    "    },\n",
    "    vectorstore = 'faiss'\n",
    ")\n",
    "\n",
    "#run_rag_evaluation(rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Retriever tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGModel_multiRetriever(weave.Model):\n",
    "    dbs_retriever_pars : dict[str, dict]\n",
    "    embeddings_name : str\n",
    "\n",
    "    vectorstore : str\n",
    "    prompt_template : str\n",
    "    chat_pars : dict[str, str|int]\n",
    "\n",
    "    output_filter : Callable\n",
    "\n",
    "    structure : str = \"multi-retriever-rag_\"  # multiple retrievers are combined for generating the context\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str):\n",
    "        retrievers = {}\n",
    "        for name, retriever_pars in self.dbs_retriever_pars.items():\n",
    "            retriever = build_retriever([name], self.embeddings_name, self.vectorstore, retriever_pars)\n",
    "            retrievers[name] = retriever\n",
    "\n",
    "        chain, llm = build_chat(self.chat_pars, self.prompt_template)\n",
    "\n",
    "        if self.vectorstore == 'faiss':\n",
    "            context = {}\n",
    "            for name, retriever in retrievers.items():\n",
    "                context[name] = retriever.invoke(question)\n",
    "\n",
    "        context = self.output_filter(context = context, question = question)\n",
    "\n",
    "        response = chain.invoke(\n",
    "            {\n",
    "                \"context\": context,\n",
    "                \"question\": question,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return {\"context\": str(context), \"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_multiret_evaluation(rag, test_dataset=test_dataset):\n",
    "    evaluation = Evaluation(\n",
    "        dataset=test_dataset, scorers=[eval_model],\n",
    "    )\n",
    "\n",
    "    with weave.attributes({\n",
    "        'dbs': list(rag.dbs_retriever_pars.keys()),\n",
    "        'embeddings': rag.embeddings_name, \n",
    "        'chat_pars': rag.chat_pars, \n",
    "        'prompt_template': rag.prompt_template, \n",
    "        'retriever_pars': rag.dbs_retriever_pars,\n",
    "        'vectorstore': rag.vectorstore, \n",
    "        \"structure\": rag.structure + rag.output_filter.__name__\n",
    "    }):\n",
    "        asyncio.run(evaluation.evaluate(rag.predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_filter = lambda context, question: context\n",
    "no_filter.__name__ = \"no_filter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_trustable_posts(context, question):\n",
    "    print(context)\n",
    "    return context\n",
    "filter_trustable_posts.__name__ = \"filter_trustable_posts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_pars = {\n",
    "    \"model\": chat_models[1],\n",
    "    \"temperature\": 0,\n",
    "    #\"max_tokens\": None,\n",
    "    #\"timeout\": None,\n",
    "    \"max_retries\": 2\n",
    "}\n",
    "\n",
    "prompt_template = f\"\"\"Answer politely the question at the end, using only the following context. The user is not necessarily a specialist, so please avoid jargon and explain any technical terms.\n",
    "\n",
    "<context>\n",
    "{{context}} \n",
    "</context>\n",
    "\n",
    "Question: {{question}}\n",
    "\"\"\"\n",
    "\n",
    "rag = RAGModel_multiRetriever(\n",
    "    dbs_retriever_pars = {\n",
    "        \"posts_forum\": {\n",
    "            \"search_kwargs\" : {\n",
    "                'k': 4,\n",
    "                'filter': {\n",
    "                    'trust_level': [2, 3, 4, 5]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"fragments_docs\": {\n",
    "            \"search_kwargs\" : {'k': 2}\n",
    "        }\n",
    "    },\n",
    "    embeddings_name = embedding_models[2],\n",
    "    chat_pars=chat_pars,\n",
    "    prompt_template = prompt_template,\n",
    "    output_filter = no_filter,\n",
    "    vectorstore = 'faiss'\n",
    ")\n",
    "\n",
    "#run_rag_multiret_evaluation(rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Compression Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "class RAGModel_contextualcompression(weave.Model):\n",
    "    structure : str = \"contextual-compression-rag\"  # use contextual compression for improving the retriever\n",
    "\n",
    "    dbs_name : list\n",
    "    embeddings_name : str\n",
    "\n",
    "    vectorstore : str\n",
    "    prompt_template : str\n",
    "    chat_pars : dict[str, str|int]\n",
    "\n",
    "    retriever_pars : dict\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str):\n",
    "        retriever = build_retriever(self.dbs_name, self.embeddings_name, self.vectorstore, self.retriever_pars)\n",
    "        chain, llm = build_chat(self.chat_pars, self.prompt_template)\n",
    "\n",
    "        if self.vectorstore == 'faiss':\n",
    "            compressor = LLMChainExtractor.from_llm(llm)\n",
    "            compression_retriever = ContextualCompressionRetriever(\n",
    "                base_compressor=compressor, base_retriever=retriever\n",
    "            )\n",
    "            context = compression_retriever.invoke(question)\n",
    "\n",
    "        response = chain.invoke(\n",
    "            {\n",
    "                \"context\": context,\n",
    "                \"question\": question,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return {\"context\": str(context), \"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_pars = {\n",
    "    \"model\": chat_models[1],\n",
    "    \"temperature\": 0,\n",
    "    #\"max_tokens\": None,\n",
    "    #\"timeout\": None,\n",
    "    \"max_retries\": 2\n",
    "}\n",
    "\n",
    "prompt_template = f\"\"\"Answer politely the question at the end, using only the following context. The user is not necessarily a specialist, so please avoid jargon and explain any technical terms.\n",
    "\n",
    "<context>\n",
    "{{context}} \n",
    "</context>\n",
    "\n",
    "Question: {{question}}\n",
    "\"\"\"\n",
    "\n",
    "rag = RAGModel_contextualcompression(\n",
    "    dbs_name = [\"full_docs\", \"threads_forum\"],\n",
    "    embeddings_name = embedding_models[2],\n",
    "    chat_pars=chat_pars,\n",
    "    prompt_template = prompt_template,\n",
    "    retriever_pars = {\n",
    "        \"search_kwargs\" : {'k': 3}\n",
    "    },\n",
    "    vectorstore = 'faiss'\n",
    ")\n",
    "\n",
    "#run_rag_evaluation(rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claude Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def build_retriever(dbs_name, embeddings_name, vectorstore = 'faiss', retriever_pars = {}):\n",
    "    db = load_db(dbs_name, embeddings_name, vectorstore)\n",
    "    if vectorstore == 'faiss':\n",
    "        retriever = db.as_retriever(**retriever_pars)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "class RAGModel_Claude(weave.Model):\n",
    "    structure : str = \"simple-rag\" # just a retriever and a llm\n",
    "\n",
    "    dbs_name : list\n",
    "    embeddings_name : str\n",
    "\n",
    "    vectorstore : str\n",
    "    retriever_pars : dict\n",
    "\n",
    "    prompt_template : Callable\n",
    "    chat_pars : dict[str, str|int]\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str):\n",
    "        retriever = build_retriever(self.dbs_name, self.embeddings_name, self.vectorstore, self.retriever_pars)\n",
    "        llm = ChatAnthropic(**self.chat_pars)\n",
    "\n",
    "        if self.vectorstore == 'faiss':\n",
    "            context = retriever.invoke(question)\n",
    "\n",
    "        response = llm.invoke(self.prompt_template(context=context, question=question))\n",
    "        \n",
    "        return {\"context\": str(context), \"answer\": response.content}\n",
    "    \n",
    "    def ask(self, question: str):\n",
    "        out = self.predict(question)\n",
    "        return out[\"answer\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_pars = {\n",
    "    \"model\": chat_models_claude[0],\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"timeout\": 60,\n",
    "    \"max_retries\": 2\n",
    "}\n",
    "\n",
    "def prompt_template(context, question):\n",
    "    return [\n",
    "        (\n",
    "            \"system\",\n",
    "            f\"You are a helpful assistant that helps access information about the Optimism Collective. Please provide polite and informative answers. Be assertive. The human is not necessarily a specialist, so please avoid jargon and explain any technical terms. \\n\\n Following there are some fragments retrieved from the Optimism Governance Forum and Optimism Documentation. This is expected to contain relevant information to answer the human question: \\n\\n {context}\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            question\n",
    "        )\n",
    "    ]\n",
    "\n",
    "rag = RAGModel_Claude(\n",
    "    dbs_name = dbs[1:3],\n",
    "    embeddings_name = embedding_models[2],\n",
    "    prompt_template = prompt_template,\n",
    "    retriever_pars = {\n",
    "        \"search_kwargs\" : {'k': 6}\n",
    "    },\n",
    "    vectorstore = 'faiss',\n",
    "    chat_pars = chat_pars\n",
    ")\n",
    "\n",
    "run_rag_evaluation(rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bleu-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
